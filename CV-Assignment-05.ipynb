{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133227e6",
   "metadata": {},
   "source": [
    "### 1. How can each of these parameters be fine-tuned? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ce90f",
   "metadata": {},
   "source": [
    "### • Number of hidden layers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059e240",
   "metadata": {},
   "source": [
    "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12203b0",
   "metadata": {},
   "source": [
    "### • Network architecture (network depth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f4958",
   "metadata": {},
   "source": [
    "Fine-tuning deep learning algorithms will help to improve the accuracy of a new neural network model by integrating data from an existing neural network and using it as an initialization point to make the training process time and resource-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d79806",
   "metadata": {},
   "source": [
    "### • Each layer's number of neurons (layer width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae9f81",
   "metadata": {},
   "source": [
    "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce9564",
   "metadata": {},
   "source": [
    "### • Form of activation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce785c10",
   "metadata": {},
   "source": [
    "form. activate() activates the form, which means if you have input elements (such as text boxes), it will focus to that particular form regardless of any other form out there. Eg. If you have shown form 1,2 and 3. And if you activate form 2, the form 2 will get focused to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc085ad6",
   "metadata": {},
   "source": [
    "### • Optimization and learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6e520",
   "metadata": {},
   "source": [
    "Approximating a function can be solved by framing the problem as function optimization. ... Machine learning algorithms perform function approximation, which is solved using function optimization. Function optimization is the reason why we minimize error, cost, or loss when fitting a machine learning algorithm\n",
    "\n",
    "Optimization is the process of improving a program's performance characteristics such as code size (compactness) and execution speed. Machine learning is the discipline of software design whose goal is to create programs that can learn how to do things on their own through learning algorithms or techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631e635",
   "metadata": {},
   "source": [
    "### • Learning rate and decay schedule\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbdc8d",
   "metadata": {},
   "source": [
    "The way in which the learning rate changes over time (training epochs) is referred to as the learning rate schedule or learning rate decay. Perhaps the simplest learning rate schedule is to decrease the learning rate linearly from a large initial value to a small value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6950b15f",
   "metadata": {},
   "source": [
    "### • Mini batch size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46140e82",
   "metadata": {},
   "source": [
    "This is known as online learning. The amount of data included in each sub-epoch weight change is known as the batch size. For example, with a training dataset of 1000 samples, a full batch size would be 1000, a mini-batch size would be 500 or 200 or 100, and an online batch size would be just 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ed0a0",
   "metadata": {},
   "source": [
    "### • Algorithms for optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2627a79",
   "metadata": {},
   "source": [
    " the importance of optimization algorithms such as stochastic gradient descent, min-batch gradient descent, gradient descent with momentum and the Adam optimizer. These methods make it possible for our neural network to learn. However, some methods perform better than others in terms of speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f73f45",
   "metadata": {},
   "source": [
    "### • The number of epochs (and early stopping criteria)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb241500",
   "metadata": {},
   "source": [
    "The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset. One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feaa923",
   "metadata": {},
   "source": [
    "### • Overfitting that be avoided by using regularization techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f79663",
   "metadata": {},
   "source": [
    "Regularization methods like weight decay provide an easy way to control overfitting for large neural network models. A modern recommendation for regularization is to use early stopping with dropout and a weight constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd82d6e8",
   "metadata": {},
   "source": [
    "### • L2 normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196af1ac",
   "metadata": {},
   "source": [
    "The L2 norm calculates the distance of the vector coordinate from the origin of the vector space. As such, it is also known as the Euclidean norm as it is calculated as the Euclidean distance from the origin. The result is a positive distance value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3aa23",
   "metadata": {},
   "source": [
    "### • Drop out layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4a801",
   "metadata": {},
   "source": [
    "The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. ... Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ea2e7",
   "metadata": {},
   "source": [
    "### • Data augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de20779",
   "metadata": {},
   "source": [
    "Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
