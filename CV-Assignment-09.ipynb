{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133227e6",
   "metadata": {},
   "source": [
    "### 1. What are the advantages of a CNN for image classification over a completely linked DNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8c214",
   "metadata": {},
   "source": [
    "The main advantage of CNN compared to its predecessors is that it automatically detects the important features without any human supervision. For example, given many pictures of cats and dogs it learns distinctive features for each class by itself. CNN is also computationally efficient.\n",
    "\n",
    "Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has many fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data.\n",
    "\n",
    "Deep NN is just a deep neural network, with a lot of layers. It can be CNN, or just a plain multilayer perceptron. CNN, or convolutional neural network, is a neural network using convolution layer and pooling layer.\n",
    "\n",
    "What is the biggest advantage utilizing CNN? Little dependence on pre processing, decreasing the needs of human effort developing its functionalities. It is easy to understand and fast to implement. It has the highest accuracy among all alghoritms that predicts images.\n",
    "\n",
    "CNNs are trained to identify and extract the best features from the images for the problem at hand. That is their main strength. The latter layers of a CNN are fully connected because of their strength as a classifier. So these two architectures aren't competing though as you may think as CNNs incorporate FC layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2901ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef777da1",
   "metadata": {},
   "source": [
    "### 2. Consider a CNN with three convolutional layers, each of which has three kernels, a stride of two, and SAME padding. The bottom layer generates 100 function maps, the middle layer 200, and the top layer 400. RGB images with a size of 200 x 300 pixels are used as input. How many criteria does the CNN have in total? How much RAM would this network need when making a single instance prediction if we're using 32-bit floats? What if you were to practice on a batch of 50 images?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bc298",
   "metadata": {},
   "source": [
    " Kernels = 3 x 3\n",
    "     Inputs Channels = 3\n",
    "     No of filters = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f975e768",
   "metadata": {},
   "source": [
    "    Input = 100  (i.e feature maps from previous layer)\n",
    "    kernel = 3 x 3\n",
    "    total weights = 3 * 3 * 100 = 900 + 1 (bias) = 901\n",
    "   So, 1 feature map has 901 weights, then 200 feature maps has 901 * 200 = 180200 parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99026fb3",
   "metadata": {},
   "source": [
    "     one computation from the previous question (4 * 903,400 = 3.4 MB) and\n",
    "     other computation was 6 + 9 = 15 million bytes (about 14.3 MB) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6fd4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d79806",
   "metadata": {},
   "source": [
    "### 3. What are five things you might do to fix the problem if your GPU runs out of memory while training a CNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ab4a2",
   "metadata": {},
   "source": [
    "If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem? Reduce the mini-batch size. Reduce dimensionality using a larger stride in one or more layers. Remove one or more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7effd556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fce9564",
   "metadata": {},
   "source": [
    "### 4. Why would you use a max pooling layer instead with a convolutional layer of the same stride?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f4d1ef",
   "metadata": {},
   "source": [
    "After a convolution operation we usually perform pooling to reduce the dimensionality. This enables us to reduce the number of parameters, which both shortens the training time and combats overfitting. Pooling layers downsample each feature map independently, reducing the height and width, keeping the depth intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa8a0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc085ad6",
   "metadata": {},
   "source": [
    "### 5. When would a local response normalization layer be useful?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ee5ab",
   "metadata": {},
   "source": [
    "Local Response Normalization (LRN) was first introduced in AlexNet architecture where the activation function used was ReLU as opposed to the more common tanh and sigmoid at that time. Apart from the reason mentioned above, the reason for using LRN was to encourage lateral inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47e11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e631e635",
   "metadata": {},
   "source": [
    "### 6. In comparison to LeNet-5, what are the main innovations in AlexNet? What about GoogLeNet and ResNet's core innovations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d11083",
   "metadata": {},
   "source": [
    "The main innovation introduced by AlexNet compared to the LeNet-5 was its sheer size. AlexNet main elements are the same: a sequence of convolutional and pooling layers followed by a couple of fully-connected layers.\n",
    "\n",
    "The main novelty in the architecture of GoogLeNet is the introduction of a particular module called Inception.\n",
    "\n",
    "The core innovation of ResNet is introducing a so-called “identity shortcut connection” that skips one or more layers,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb200af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6950b15f",
   "metadata": {},
   "source": [
    "### 7. On MNIST, build your own CNN and strive to achieve the best possible accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of loading the mnist dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "# load dataset\n",
    "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    "\t# define subplot\n",
    "\tplt.subplot(330 + 1 + i)\n",
    "\t# plot raw pixel data\n",
    "\tplt.imshow(trainX[i], cmap=plt.get_cmap('gray'))\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a619bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train: X=(60000, 28, 28), y=(60000,)\n",
    "Test: X=(10000, 28, 28), y=(10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372125c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d19a29bf",
   "metadata": {},
   "source": [
    "### 8. Using Inception v3 to classify broad images. a.\n",
    "Images of different animals can be downloaded. Load them in Python using the matplotlib.image.mpimg.imread() or scipy.misc.imread() functions, for example. Resize and/or crop them to 299 x 299 pixels, and make sure they only have three channels (RGB) and no transparency. The photos used to train the Inception model were preprocessed to have values ranging from -1.0 to 1.0, so make sure yours do as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "!pip install tf-slim\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tf_slim as slim\n",
    "from tf_slim.nets import inception\n",
    "import tf_slim as slim\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Data\n",
    "\n",
    "ckpt_path = \"/kaggle/input/inception_v3.ckpt\"\n",
    "images_path = \"/kaggle/input/animals/*\"\n",
    "img_width = 299\n",
    "img_height = 299\n",
    "batch_size = 16\n",
    "batch_shape = [batch_size, img_height, img_width, 3]\n",
    "num_classes = 1001\n",
    "predict_output = []\n",
    "class_names_path = \"/kaggle/input/imagenet_class_names.txt\"\n",
    "with open(class_names_path) as f:\n",
    "    class_names = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3227ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Inception v3 model\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=batch_shape)\n",
    "\n",
    "with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "    logits, end_points = inception.inception_v3(\n",
    "        X, num_classes=num_classes, is_training=False\n",
    "    )\n",
    "\n",
    "predictions = end_points[\"Predictions\"]\n",
    "saver = tf.train.Saver(slim.get_model_variables())\n",
    "Define function for loading images and resizing for sending to model for evaluation in RGB mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b4708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(input_dir):\n",
    "    global batch_shape\n",
    "    images = np.zeros(batch_shape)\n",
    "    filenames = []\n",
    "    idx = 0\n",
    "    batch_size = batch_shape[0]\n",
    "    files = tf.gfile.Glob(input_dir)[:20]\n",
    "    files.sort()\n",
    "    for filepath in files:\n",
    "        with tf.gfile.Open(filepath, \"rb\") as f:\n",
    "            imgRaw = np.array(Image.fromarray(imread(f, as_gray=False, pilmode=\"RGB\")).resize((299, 299))).astype(np.float) / 255.0\n",
    "             # Images for inception classifier are normalized to be in [-1, 1] interval.\n",
    "        images[idx, :, :, :] = imgRaw * 2.0 - 1.0\n",
    "        filenames.append(os.path.basename(filepath))\n",
    "        idx += 1\n",
    "        if idx == batch_size:\n",
    "            yield filenames, images\n",
    "            filenames = []\n",
    "            images = np.zeros(batch_shape)\n",
    "            idx = 0\n",
    "    if idx > 0:\n",
    "        yield filenames, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2804302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-Trained Model\n",
    "\n",
    "session_creator = tf.train.ChiefSessionCreator(\n",
    "        scaffold=tf.train.Scaffold(saver=saver),\n",
    "        checkpoint_filename_with_path=ckpt_path,\n",
    "        master='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify Images using Model\n",
    "\n",
    "with tf.train.MonitoredSession(session_creator=session_creator) as sess:\n",
    "    for filenames, images in load_images(images_path):\n",
    "        labels = sess.run(predictions, feed_dict={X: images})\n",
    "        for filename, label, image in zip(filenames, labels, images):\n",
    "            predict_output.append([filename, label, image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d2eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Predicted Output\n",
    "\n",
    "for x in predict_output:\n",
    "    out_list = list(x[1])\n",
    "    topPredict = sorted(range(len(out_list)), key=lambda i: out_list[i], reverse=True)[:5]\n",
    "    plt.imshow((((x[2]+1)/2)*255).astype(int))\n",
    "    plt.show()\n",
    "    print(\"Filename:\",x[0])\n",
    "    print(\"Displaying the top 5 Predictions for above image:\")\n",
    "    for p in topPredict:\n",
    "        print(class_names[p-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81ace7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56dc82d9",
   "metadata": {},
   "source": [
    "### 9. Large-scale image recognition using transfer learning.\n",
    "a. Make a training set of at least 100 images for each class. You might, for example, identify your own photos based on their position (beach, mountain, area, etc.) or use an existing dataset, such as the flowers dataset or MIT's places dataset (requires registration, and it is huge).\n",
    "b. Create a preprocessing phase that resizes and crops the image to 299 x 299 pixels while also adding some randomness for data augmentation.\n",
    "c. Using the previously trained Inception v3 model, freeze all layers up to the bottleneck layer (the last layer before output layer) and replace output layer with  appropriate number of outputs for your new classification task (e.g., the flowers dataset has five mutually exclusive classes so the output layer must have five neurons and use softmax activation function).\n",
    "d. Separate the data into two sets: a training and a test set. The training set is used to train the model, and the test set is used to evaluate it.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
