{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133227e6",
   "metadata": {},
   "source": [
    "### 1. Why don't we start all of the weights with zeros?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20eddc3",
   "metadata": {},
   "source": [
    "Initializing all the weights with zeros leads the neurons to learn the same features during training. In fact, any constant initialization scheme will perform very poorly. ... Thus, both neurons will evolve symmetrically throughout training, effectively preventing different neurons from learning different things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7124f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12203b0",
   "metadata": {},
   "source": [
    "### 2. Why is it beneficial to start weights with a mean zero distribution?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc86c5c4",
   "metadata": {},
   "source": [
    "When there is no change in the Output, there is no gradient and hence no direction. Main problem with initialization of all weights to zero mathematically leads to either the neuron values are zero (for multi layers) or the delta would be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcd200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d79806",
   "metadata": {},
   "source": [
    "### 3. What is dilated convolution, and how does it work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e294df7",
   "metadata": {},
   "source": [
    "Dilated Convolution: It is a technique that expands the kernel (input) by inserting holes between the its consecutive elements. In simpler terms, it is same as convolution but it involves pixel skipping, so as to cover a larger area of the input.\n",
    "\n",
    "The more important point is that the architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. Allows one to have larger receptive field with same computation and memory costs while also preserving resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c056c53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fce9564",
   "metadata": {},
   "source": [
    "### 4. What is TRANSPOSED CONVOLUTION, and how does it work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb23642",
   "metadata": {},
   "source": [
    "Transposed convolutions are standard convolutions but with a modified input feature map. The stride and padding do not correspond to the number of zeros added around the image and the amount of shift in the kernel when sliding it across the input, as they would in a standard convolution operation.\n",
    "\n",
    "conv2d_transpose() simply transposes the weights and flips them by 180 degrees. Then it applies the standard conv2d(). \"Transposes\" practically means that it changes the order of the \"columns\" in the weights tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12e51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc085ad6",
   "metadata": {},
   "source": [
    "### 5.Explain Separable convolution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd4bf5",
   "metadata": {},
   "source": [
    "The spatial separable convolution is so named because it deals primarily with the spatial dimensions of an image and kernel: the width and the height. (The other dimension, the “depth” dimension, is the number of channels of each image). A spatial separable convolution simply divides a kernel into two, smaller kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf195a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e631e635",
   "metadata": {},
   "source": [
    "### 6.What is depthwise convolution, and how does it work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20e034",
   "metadata": {},
   "source": [
    "Depthwise Convolution is a type of convolution where we apply a single convolutional filter for each input channel. In the regular 2D convolution performed over multiple input channels, the filter is as deep as the input and lets us freely mix channels to generate each element in the output.\n",
    "\n",
    "Depthwise Convolution is a type of convolution where we apply a single convolutional filter for each input channel.\n",
    "To summarize the steps, we:\n",
    "Split the input and filter into channels.\n",
    "We convolve each input with the respective filter.\n",
    "We stack the convolved outputs together.\n",
    "\n",
    "Unlike spatial separable convolutions, depthwise separable convolutions work with kernels that cannot be “factored” into two smaller kernels. ... The depthwise separable convolution is so named because it deals not just with the spatial dimensions, but with the depth dimension — the number of channels — as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf21457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6950b15f",
   "metadata": {},
   "source": [
    "### 7.What is Depthwise separable convolution, and how does it work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da7a665",
   "metadata": {},
   "source": [
    "Unlike spatial separable convolutions, depthwise separable convolutions work with kernels that cannot be “factored” into two smaller kernels. ... The depthwise separable convolution is so named because it deals not just with the spatial dimensions, but with the depth dimension — the number of channels — as well.\n",
    "\n",
    "In convolutional neural networks (CNN), 2D convolutions are the most frequently used convolutional layer. MobileNet is a CNN architecture that is much faster as well as a smaller model that makes use of a new kind of convolutional layer, known as Depthwise Separable convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2354423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "092ed0a0",
   "metadata": {},
   "source": [
    "### 8.Capsule networks are what they sound like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40ed21a",
   "metadata": {},
   "source": [
    "A Capsule Neural Network (CapsNet) is a machine learning system that is a type of artificial neural network (ANN) that can be used to better model hierarchical relationships. The approach is an attempt to more closely mimic biological neural organization.\n",
    "\n",
    "Capsule networks (CapsNets), a new class of deep neural network architectures proposed recently by Hinton et al., have shown a great performance in many fields, particularly in image recognition and natural language processing.\n",
    "\n",
    "Why don't we use Capsule Networks? While CapsNet has achieved state of the art performance on simple datasets such as MNIST, it struggles on more complex data that might be found on datasets such as CIFAR-10 or Imagenet. This is because of the excess amount of information that is found in images throw off the capsules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32193c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72f73f45",
   "metadata": {},
   "source": [
    "### 9. Why is POOLING such an important operation in CNNs?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94dbf2",
   "metadata": {},
   "source": [
    "Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer\n",
    "\n",
    "Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244a8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4feaa923",
   "metadata": {},
   "source": [
    "### 10. What are receptive fields and how do they work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbc5bd1",
   "metadata": {},
   "source": [
    "The receptive field encompasses the sensory receptors that feed into sensory neurons and thus includes specific receptors on a neuron as well as collectives of receptors that are capable of activating a neuron via synaptic connections.\n",
    "\n",
    "Analogously, the receptive field of a neuron is the portion of the sensory field that affects the signalling of that neuron. For instance, the entire area that an eye can see is called the visual field, and the patch of the visual field that any single neuron monitors is that cell's receptive field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acaf55f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
