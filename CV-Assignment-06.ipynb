{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133227e6",
   "metadata": {},
   "source": [
    "### 1. What is the difference between TRAINABLE and NON-TRAINABLE PARAMETERS?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a848fb",
   "metadata": {},
   "source": [
    "Trainable parameters are the number of, well, trainable elements in your network; neurons that are affected by backpropagation. For example, for the Wx + b operation in each neuron, W and b are trainable â€“ because they are changed by optimizers after backpropagation was applied for gradient computation.\n",
    "\n",
    "non-trainable parameters of a model are those that you will not be updating and optimized during training, and that have to be defined a priori, or passed as inputs. The example of such parameters are: the number of hidden layers. nodes on each hidden layer. the nodes on each individual layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bddc472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12203b0",
   "metadata": {},
   "source": [
    "### 2. In the CNN architecture, where does the DROPOUT LAYER go?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae82a1e",
   "metadata": {},
   "source": [
    "Dropout can be used after convolutional layers (e.g. Conv2D) and after pooling layers (e.g. MaxPooling2D). Often, dropout is only used after the pooling layers, but this is just a rough heuristic. In this case, dropout is applied to each element or cell within the feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288ba31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d79806",
   "metadata": {},
   "source": [
    "### 3. What is the optimal number of hidden layers to stack?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3349c9ba",
   "metadata": {},
   "source": [
    "There is currently no theoretical reason to use neural networks with any more than two hidden layers. In fact, for many practical problems, there is no reason to use any more than one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a526d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fce9564",
   "metadata": {},
   "source": [
    "### 4. In each layer, how many secret units or filters should there be?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e220f8e",
   "metadata": {},
   "source": [
    "For example, it is common for a convolutional layer to learn from 32 to 512 filters in parallel for a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5bb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc085ad6",
   "metadata": {},
   "source": [
    "### 5. What should your initial learning rate be?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6134c8",
   "metadata": {},
   "source": [
    "Conversely, larger learning rates will require fewer training epochs. Further, smaller batch sizes are better suited to smaller learning rates given the noisy estimate of the error gradient. A traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7bf3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e631e635",
   "metadata": {},
   "source": [
    "### 6. What do you do with the activation function?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada1cb7",
   "metadata": {},
   "source": [
    "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1cf4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6950b15f",
   "metadata": {},
   "source": [
    "### 7. What is NORMALIZATION OF DATA?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff53723",
   "metadata": {},
   "source": [
    "Data normalization is generally considered the development of clean data. Diving deeper, however, the meaning or goal of data normalization is twofold:\n",
    "\n",
    "1. Data normalization is the organization of data to appear similar across all records and fields.\n",
    "2. It increases the cohesion of entry types leading to cleansing, lead generation, segmentation, and higher quality data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556587f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "092ed0a0",
   "metadata": {},
   "source": [
    "### 8. What is IMAGE AUGMENTATION and how does it work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c5725",
   "metadata": {},
   "source": [
    "Image augmentation is a technique of altering the existing data to create some more data for the model training process. In other words, it is the process of artificially expanding the available dataset for training a deep learning model.\n",
    "\n",
    "Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset. ... Image data augmentation is used to expand the training dataset in order to improve the performance and ability of the model to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222d9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72f73f45",
   "metadata": {},
   "source": [
    "### 9. What is DECLINE IN LEARNING RATE?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba089cd",
   "metadata": {},
   "source": [
    "Perhaps the simplest learning rate schedule is to decrease the learning rate linearly from a large initial value to a small value. This allows large weight changes in the beginning of the learning process and small changes or fine-tuning towards the end of the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb8c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4feaa923",
   "metadata": {},
   "source": [
    "### 10. What does EARLY STOPPING CRITERIA mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98764c6b",
   "metadata": {},
   "source": [
    "Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset. In this tutorial, you will discover the Keras API for adding early stopping to overfit deep learning neural network models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
